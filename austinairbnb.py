# -*- coding: utf-8 -*-
"""austinairbnb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QBV5QMlLB5tTeNhe1x9YSIdXM80cV-if
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython import display
# %matplotlib inline
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

dfaustincal = pd.read_csv('/content/gdrive/MyDrive/AUSTIN/calendar.csv')
dfaustincal.head()

#listings dataframe
dfauslistings = pd.read_csv('/content/gdrive/MyDrive/AUSTIN/listings.csv')
dfauslistings.head()

# reviews dataframe
dfausreviews = pd.read_csv('/content/gdrive/MyDrive/AUSTIN/reviews-2.csv')
dfausreviews.head()

print("rows and columns of dataset", dfaustincal.shape)
print(dfaustincal.isnull().sum())
print(dfaustincal.describe(include='all'))

print("rows and columns of dataset ", dfauslistings.shape)
print(dfauslistings.isnull().sum())
print(dfauslistings.describe(include='all'))
dfauslistings

print("rows and columns of dataset ", dfausreviews.shape)
print(dfausreviews.isnull().sum())
print(dfausreviews.describe(include='all'))

hostresp=dfauslistings['host_response_time'].value_counts()

(hostresp/dfauslistings.shape[0]).plot(kind="bar", color=['green', 'red', 'red', 'red'])

plt.title("Host Response Time")

propertytype = dfauslistings['room_type'].value_counts()
(propertytype/dfauslistings.shape[0]).plot(kind="bar")
plt.title("Property Type")

dfaustincal['year'] = pd.DatetimeIndex(dfaustincal['date']).year
dfaustincal['month'] = pd.DatetimeIndex(dfaustincal['date']).month

# float and removing $
dfaustincal['price'] = dfaustincal['price'].replace('[\$,]', '', regex=True).astype(float)

dfaustincal.groupby(['year','month'])[['price']].mean().plot(kind="bar")

ziparea= pd.read_csv('/content/gdrive/MyDrive/AUSTIN/csvData.csv')
ziparea

dfauslistings['price'] = dfauslistings['price'].replace('[\$,]', '', regex=True).astype(float)

print(dfauslistings.groupby(['neighbourhood'])[['price']].mean().sort_values(by='price',ascending=False))
dfauslistings.groupby(['neighbourhood'])[['price']].mean().plot()
print(dfauslistings.neighbourhood.value_counts())

# average price by neighborhood


dfauslistings['price'] = dfauslistings['price'].replace('[\$,]', '', regex=True).astype(float)

y=dfauslistings.groupby(['neighbourhood_cleansed'])[['price']].mean().sort_values(by='price',ascending=False)
y.rename(columns = {'neighbourhood_cleansed':'zip'}, inplace = True)

toplocal3=dfauslistings.groupby(['neighbourhood'])[['price']].mean().sort_values(by='price',ascending=False).head()
print(toplocal3)
toplocal3.plot(kind='bar')

taillocal=dfauslistings.groupby(['neighbourhood'])[['price']].mean().sort_values(by='price',ascending=False).tail()
taillocal.plot(kind='bar');

neigh = ('Cypress Mill, Texas, United States','Burnet County, Texas, United States','Rollingwood, Texas, United States','Lake Travis, Texas, United States','Spicewood , Texas, United States')
prop = ('Entire home/apt','Private room')
df_1 = dfauslistings.loc[dfauslistings['neighbourhood'].isin(neigh)]
df_2 = df_1.loc[dfauslistings['room_type'].isin(prop)]

df_prop = df_2.groupby(['neighbourhood','room_type'])[['price']].mean().reset_index()
df_prop

comments = dfausreviews[['listing_id', 'comments']]
print(comments.head())

def get_neigh(listing_id):
    
    neighbourhood = dfauslistings.loc[dfauslistings['id'] == listing_id, 'neighbourhood']
    return neighbourhood

# Test function
get_neigh(21126)

y=comments.merge(dfauslistings,left_on='listing_id',right_on='id')

y

y=y[['listing_id','comments','neighbourhood']]

import nltk
nltk.downloader.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

analyser = SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(sentence):
   
    sentence = str(sentence)
    score = analyser.polarity_scores(sentence)
    return score['compound']


y['polarity_score'] = y['comments'].apply(sentiment_analyzer_scores)
y.head(20)

y.tail(20)

y=y.dropna()
y

print('Positive scores: ',y['polarity_score'][y['polarity_score'] > 0].count())
print('Negative  scores: ',y['polarity_score'][y['polarity_score'] < 0].count())
print('Neutral  scores: ',y['polarity_score'][y['polarity_score'] == 0].count())

neigh_polarity = y.groupby('neighbourhood')[['polarity_score']].mean().sort_values(by='polarity_score',ascending=False)

neigh_polarity.head(20).plot(kind='bar');

neigh_polarity.tail(20).plot(kind='bar');

neigh_polarity.plot(kind='bar');

y.sort_values(by='polarity_score',ascending=True).tail(10)

y.sort_values(by='polarity_score',ascending=False).tail(30)

a=y.groupby('listing_id')
a